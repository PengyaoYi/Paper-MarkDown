# 统计学习方法

## 概论

统计学习基于数据构建概率统计模型并运用模型对数据进行预测与分析。

可分为三个部分

1. 监督学习：从标注数据中学习预测模型
2. 无监督学习：从无标注数据中学习预测模型的机器学习问题，学习数据中的统计规律或潜在结构
3. 强化学习：智能系统在与环境连续互动过程中学习最有行为策略的机器学习问题

**概率模型与非概率模型**

概率模型取条件概率分布形式$P(y|x)$,非概率模型取函数形式$y=f(x)$,在监督学习中，概率模型是**生成模型**，非概率模型是**判别模型**

概率模型：决策树、朴素贝叶斯、隐马尔科夫模型、条件随机场、概率潜在语义分析、潜在迪利克雷分布、高斯混合模型。

非概率模型：感知机、支持向量机、k近邻、AdaBoost、k均值潜在语义分析、神经网络是非概率模型

区别：模型内在结构。概率模型一定可以表示为联合概率分布的形式，其中的变量表示

**在线学习（online learning）和批量学习(batch learning)**

在线学习：每次接受一个样本进行预测，然后学习模型，然后不断重复

批量学习：一次接受所有数据，然后进行学习

**常用损失函数**

0-1损失函数：
$$
L(Y,f(x))=\left\{
\begin{aligned}
1 ，&  Y\neq f(X) \\
0 ，&  Y\neq f(X) \\
\end{aligned}
\right.
$$


平方损失函数：
$$
L(Y,f(X))=（Y-f(X）)^2
$$


绝对损失函数：
$$
L(Y,f(X))=|Y-f(X)|
$$


对数损失函数：
$$
L(Y,P(Y|X))=-logP(Y|X)
$$

 ## 正则化：

选择经验风险与模型复杂度同时较小的模型

L1范数：$\lambda||\omega||_1$ 参数向量绝对值之和，趋于选择更少的特征

L2范数：$\frac \lambda 2||\omega||^2$ 各个元素平方的和，避免过拟合，提高泛化能力，计算方便

## 感知机

输入空间为实数域，输出空间是+1 -1，感知机是
$$
f(x)=sign(w\cdot x+b)
$$
其中sign(x)是符号函数，当x>=0 sign(x)=1;当x<0 sign(x)=-1.

感知机是一种线性分类模型，属于判别模型。构造了一个特征空间中的超平面，把平面上的样本分为正负两类。

### 学习策略

假设数据是线性可分的，为了确定参数w和b需要指定一个学习策略，即定义损失函数并使损失函数最小化

任意一个点$x_0$到超平面S的距离：
$$
-\frac{1}{||\omega||}y_i(\omega\cdot x_i+b)
$$
定义感知机的损失函数为
$$
L(\omega,b)=-\sum\limits_{x_i\in M}y_i(\omega\cdot x_i+b)
$$

### 学习算法

给定训练数据集：
$$
T=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}
$$
其中，$x_i \in R^n, y_i \in \{-1,1\}, i=1,2,...,N，求参数\omega,b 使以下损失函数最小化$：
$$
\mathop{min}\limits_{\omega,b}=-\sum\limits_{x_i\in M}y_i(\omega\cdot x_i+b)
$$
使用**随机梯度下降法**，任取一个超平面$w_0,b_0$,然后使用梯度下降法。不是选取所有误分类点而是一次随机选取一个误分雷电使其梯度下降。

> 对偶算法还没看

## k近邻

给定数据集
$$
T=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}
$$
其中x是实例的特征向量 y是实例的类别。

输出实例x所属类别y

根据给定的**距离度量方法**，在训练集T中找到与x最近的k个点，涵盖这k个点的x的邻域被标记为$N_k(x)$,在该区域中的点根据**分类决策规则**确定x的类别y。当k=1时被称为最近邻。该算法没有显示学习的过程。

k近邻所用模型即对于特征空间的划分。

三要素：**距离度量，k值选择和分类决策规则**

#### 距离度量

$L_p$距离
$$
L_p(x_i,y_i)=(\sum\limits_{l=1}^n|x_i^{(l)}-x_j^{l}|^p)^{\frac 1p}
$$
其中$p\ge1$

* p=2时，欧氏距离
  $$
  L_2(x_i,y_i)=(\sum\limits_{l=1}^n|x_i^{(l)}-x_j^{l}|^2)^{\frac 12}
  $$
  

* p=1时，曼哈顿距离
  $$
  L_1(x_i,y_i)=(\sum\limits_{l=1}^n|x_i^{(l)}-x_j^{l}|)
  $$
  

* $p=\infty$时，坐标距离的最大值

* Minkowski距离

#### k值选择

k小学习近似误差会小，但估计误差会很大，模型敏感；k大则相反，模型简单。

#### 分类决策规则

多数表决规则

> KD树

## 朴素贝叶斯

基于贝叶斯定理和一个强假设条件：特征条件独立假设

基本方法：

输入空间$\chi \subseteq R^n$ 是n维向量的集合， 输出空间为类标记的集合$\gamma = \{c_1,c_2,...,c_n\}$，输入为特征向量$x\in\chi$,输出为类标记$y \in \gamma$ X,Y是随机变量。

训练数据集
$$
T=\{(x_1,y_1),(x2,y2),...,(x_N,y_N)\}
$$
由P(X,Y)独立同分布产生

​	朴素贝叶斯通过训练数据学习P(X,Y)联合概率分布，首先学习先验分布：
$$
P(Y=c_k),\quad \quad k=1,2,...,K
$$
​	条件概率分布：
$$
P(X=x|Y=c_k)=P(X^{(1)}=x^{(1)},...,X^{(n)}=x^{(n)}|Y=c_k),\quad\quad k=1,2,...,K
$$
然后学习到联合概率分布P(X,Y)

然而根据贝叶斯公式有：
$$
P(X^{(1)}=x^{(1)},...,X^{(n)}=x^{(n)},Y=c_k)=\\
P(X^{(1)}=x^{(1)})\cdot P(X^{(2)}=x^{(2)}|X^{(1)}=x^{(1)})...P(Y=c_k|X^{(1)}=x^{(1)},X^{(2)}=x^{(2)},...,X^{(n)}=x^{(n)})
$$
难以计算，因此做出条件独立性假设，即特征向量之间是独立的，因此公式变为
$$
P(X^{(1)}=x^{(1)},...,X^{(n)}=x^{(n)},Y=c_k)=\prod \limits_{j=1}^n	P(X^{(j)}=x^{(j)})\cdot P(Y=c_k)
$$
因此朴素贝叶斯的后验概率为：
$$
P(Y=c_k|X=x)=\frac{\prod \limits_{j=1}^n	P(X^{(j)}=x^{(j)}|Y=c_k)\cdot P(Y=c_k)}{\sum \limits_k P(Y=c_k)\prod \limits_{j=1}^n	P(X^{(j)}=x^{(j)}|Y=c_k)}
$$
朴素贝叶斯分类器可表示为：
$$
y=f(x)=arg \max_{c_k}\frac{\prod \limits_{j=1}^n	P(X^{(j)}=x^{(j)}|Y=c_k)\cdot P(Y=c_k)}{\sum \limits_k P(Y=c_k)\prod \limits_{j=1}^n	P(X^{(j)}=x^{(j)}|Y=c_k)}
$$
其中分母部分对于每一个$c_k$都是相同的，因此可以简化为：
$$
y=f(x)=arg \max_{c_k} P(Y=c_k)\cdot\prod \limits_{j=1}^n	P(X^{(j)}=x^{(j)}|Y=c_k)
$$


朴素贝叶斯算法：

1. 计算先验概率和条件概率
   $$
   p(Y=c_k)\\
   P(X^{(j)}=a_{jl}|Y=c_k)\\
   对于给定的实例x，计算:\\
   P(Y=c_k)\prod \limits_{j=1}^n	P(X^{(j)}=x^{(j)}|Y=c_k),\quad\quad k=1,2,...,K\\
   最后确定x的类别\\
   y=arg \max_{c_k} P(Y=c_k)\cdot\prod \limits_{j=1}^n	P(X^{(j)}=x^{(j)}|Y=c_k)
   $$

   > 贝叶斯估计 所有概率的地方加个$\lambda$ 避免概率0影响后验概率计算（拉普拉斯平滑）

## 决策树

 

## 逻辑斯蒂回归和最大熵

## 支持向量机

## 提升方法

## EM算法

## HMM

## CRF

## 无监督学习概论

## 聚类

## 奇异值分解

## 主成分分析

## 潜在语义分析

## 马尔科夫蒙特卡洛方法

## 潜在迪利克雷分布

## PageRank

## 总结



