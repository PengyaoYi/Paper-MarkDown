# BOOSTING

##### 思想

对于一个复杂任务而言，多个专家判断综合结果肯定比单个专家判断得好。

**强可学习**：在概率近似正确（PAC）学习框架中，如果存在一个多项式学习算法可以学习某个类，那么这个类是强可学习的。

**弱可学习**： 一个概念（一个类）如果存在一个多项式的学习算法可以学习它，但算法学习的准确率只比随机猜测略好，那么这个概念就是弱可学习的

在PAC学习框架中 强可学习和弱可学习是等价的。因此如果发现了一种弱学习算法，则可将它提升（**BOOST**）为强可学习。对于分类问题而言，我们可以较为简单的求取比较粗糙的分类规则（弱分类器），然后从弱学习算法出发，反复学习，得到一系列弱分类器（基本分类器），然后组合这些分类器，构成一个强分类器。大多数提升方法是改变训练数据的概率分布，针对不同训练数据分布条用弱学习算法学习一系列弱分类器。

1. 如何在每轮学习中改变训练数据的权值或者概率分布。
2. 如何将弱分类器组合成为强分类器。

## AdaBoost

1. 提高那些前一轮弱分类器错误分类样本的权重，降低那些被正确分类的样本权值。
2. 加权多数表决。加大分类误差率小的弱分类器的权值。使其在表决中起较大作用；减小分类错误率大的弱分类器的权值。

### 算法：

---

**输入**：

训练集$ T={(x_1,y_1),(x_2,y_2),...,(x_m,y_m)}$

训练轮数T

**输出**：最终分类器$G(x)$

**过程**：

1. $D_1(x)=1/m$ 初始化训练数据权值分布

2. 对m=1，2，,...,M

   1. 使用具有权值分布$D_m$的训练集进行学习得到基本分类器$G_m$

   2. 计算$G_m$在训练集上的分类误差率：
      $$
      e_m=\sum_{i=1}^{N}P(G_m(x_i)\neq y_i)
      $$

   3. 计算$G_m(x)$的系数
      $$
      \alpha_m=\frac{1}{2}log\frac{1-e_m}{e_m}
      $$

   4. 更新训练数据集权重分布
      $$
      D_{m+1}=(w_{m+1,1},...,w_{m+1,N})\\
      w_{m+1,i}=\frac{w_{mi}}{Z_m}exp(-\alpha_my_iG_m(x_i))
      $$
      

      ​    其中$Z_m$是归一化因子

3. 构建基本分类器的线性组合
   $$
   f(x)=\sum_{m=1}^{M}\alpha_mG_m(x)
   $$
   最终分类器是：
   $$
   G(x)=sign(f(x))
   $$
   

## GBDT

提升树模型可以表示为决策树的加法模型：
$$
f_M(x)=\sum_{m=1}^{M}T(x;\Theta_m)
$$
Boost方法分为加法模型（基函数线性组合）与前向分布算法。