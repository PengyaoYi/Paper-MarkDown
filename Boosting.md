# BOOSTING

##### 思想

对于一个复杂任务而言，多个专家判断综合结果肯定比单个专家判断得好。

**强可学习**：在概率近似正确（PAC）学习框架中，如果存在一个多项式学习算法可以学习某个类，那么这个类是强可学习的。

**弱可学习**： 一个概念（一个类）如果存在一个多项式的学习算法可以学习它，但算法学习的准确率只比随机猜测略好，那么这个概念就是弱可学习的

在PAC学习框架中 强可学习和弱可学习是等价的。因此如果发现了一种弱学习算法，则可将它提升（**BOOST**）为强可学习。对于分类问题而言，我们可以较为简单的求取比较粗糙的分类规则（弱分类器），然后从弱学习算法出发，反复学习，得到一系列弱分类器（基本分类器），然后组合这些分类器，构成一个强分类器。大多数提升方法是改变训练数据的概率分布，针对不同训练数据分布条用弱学习算法学习一系列弱分类器。

1. 如何在每轮学习中改变训练数据的权值或者概率分布。
2. 如何将弱分类器组合成为强分类器。

## AdaBoost

1. 提高那些前一轮弱分类器错误分类样本的权重，降低那些被正确分类的样本权值。
2. 加权多数表决。加大分类误差率小的弱分类器的权值。使其在表决中起较大作用；减小分类错误率大的弱分类器的权值。

### 算法：

---

**输入**：

训练集$ T={(x_1,y_1),(x_2,y_2),...,(x_m,y_m)}$

训练轮数T

**输出**：最终分类器$G(x)$

**过程**：

1. $D_1(x)=1/m$ 初始化训练数据权值分布

2. 对m=1，2，,...,M

   1. 使用具有权值分布$D_m$的训练集进行学习得到基本分类器$G_m$

   2. 计算$G_m$在训练集上的分类误差率：
      $$
      e_m=\sum_{i=1}^{N}P(G_m(x_i)\neq y_i)
      $$

   3. 计算$G_m(x)$的系数
      $$
      \alpha_m=\frac{1}{2}log\frac{1-e_m}{e_m}
      $$

   4. 更新训练数据集权重分布
      $$
      D_{m+1}=(w_{m+1,1},...,w_{m+1,N})\\
      w_{m+1,i}=\frac{w_{mi}}{Z_m}exp(-\alpha_my_iG_m(x_i))
      $$
      

      ​    其中$Z_m$是归一化因子

3. 构建基本分类器的线性组合
   $$
   f(x)=\sum_{m=1}^{M}\alpha_mG_m(x)
   $$
   最终分类器是：
   $$
   G(x)=sign(f(x))
   $$
   

## GBDT

提升树模型可以表示为决策树的加法模型：
$$
f_M(x)=\sum_{m=1}^{M}T(x;\Theta_m)
$$
Boost方法分为加法模型（基函数线性组合）与前向分布算法。

第m步模型是：
$$
f_m(x)=f_{m-1}(x)+T(x;\Theta_m)\\
\hat\Theta_m=arg\mathop{min} \limits_{\theta_m}\sum_{i=1}^NL(y_i,f_{m-1}(x_i)+T(x_i;\Theta_m))
$$
**对于回归问题的提升树算法**

一般使用平方误差损失函数，
$$
L(y,f(x))=(y-f(x))^2\\
L(y,f_{m-1}(x)+T(x;\Theta_m))=[y-f_{m-1}(x)-T(x;\Theta_m)]^2
=[r-T(x;\Theta_m)]^2\\
其中r=y-f_{m-1}(x)
$$


输入：训练数据集$T={(x_1,y_1),(x_2,y_2),...,(x_N,y_N)}$

输出：提升树$f_M(x)$

1. 初始化$f_0(x)=0$
2. 对于m=0,1,2,...,M:
   1. 计算残差r
   2. 拟合残差学习回归树得到$T(x;\Theta_m)$
   3. 更新$f_m(x)=f_{m-1}(x)+T(x;\Theta_m)$

3. 得到回归提升树$f_M(x)=\sum\limits_{m=1}^{M}T(x;\Theta_m)$

**GBDT**

输入：训练数据集$T={(x_1,y_1),(x_2,y_2),...,(x_N,y_N)}$

输出：提升树$f_M(x)$

1. 初始化$f_0(x)=arg\mathop{min}\limits_{c}\sum\limits_{i=1}^NL(y_i,c)$

2. 对于m=0,1,2,...,M:

   1. 计算残差r
      $$
      r_{mi}=-[\frac{\partial L(y_i,f(x_i))}{\partial f(x_i)}]_{f(x)=f_{m-1}(x)}
      $$
      

   2. 拟合残差学习回归树得到第m棵树的叶节点区域$R_{mj},j=1,2,...,J$

   3. 对$j=1,2,...,J$,计算
      $$
      c_{mj}=arg\mathop{min}\limits_c\sum\limits_{x_i\in R_mj}L(y_i,f_{m-1}(x_i)+c)
      $$
      

   4. 更新$f_m(x)=f_{m-1}(x)+\sum\limits_{j=1}^{J}c_{mj}I(x\in R_{mj})$

3. 得到回归提升树$f_M(x)=\sum\limits_{m=1}^{M}\sum\limits_{j=1}^{J}c_{mj}I(x\in R_{mj})$

## XGBoost

